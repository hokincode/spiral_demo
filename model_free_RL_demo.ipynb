{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Game Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class MazeGame:\n",
    "    def __init__(self):\n",
    "        maze_layout = [[1, 1, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 0, 1],\n",
    "               [1, 0, 1, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 0],\n",
    "               [1, 1, 1, 1, 1, 1, 1]]\n",
    "        \"\"\"Initialize the maze game with a given layout.\"\"\"\n",
    "        self.layout = np.array(maze_layout)\n",
    "        self.rgb_maze = self._to_rgb()\n",
    "        self.start_pos = None\n",
    "        self.exit_pos = None\n",
    "        self.current_pos = None\n",
    "\n",
    "    def _to_rgb(self):\n",
    "        \"\"\"Convert the maze layout to an RGB image.\"\"\"\n",
    "        rgb = np.zeros((*self.layout.shape, 3))  # Create a new RGB array\n",
    "        for i in range(3):  # Copy the grayscale values into each RGB channel\n",
    "            rgb[:, :, i] = 1 - self.layout\n",
    "        return rgb\n",
    "\n",
    "    def initialPosition(self, row, col):\n",
    "        \"\"\"Set the initial position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.start_pos = (row, col)\n",
    "            self.current_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 0, 1])  # Mark initial position with blue\n",
    "        else:\n",
    "            print(\"Invalid initial position: It is on a wall.\")\n",
    "\n",
    "    def setExit(self, row, col):\n",
    "        \"\"\"Set the exit position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.exit_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 1, 0])  # Mark exit position with green\n",
    "        else:\n",
    "            print(\"Invalid exit position: It is on a wall.\")\n",
    "\n",
    "    def set_point(self, row, col, color):\n",
    "        \"\"\"Set a specific point in the maze to a given color.\"\"\"\n",
    "        self.rgb_maze[row, col] = color\n",
    "\n",
    "    def makeMove(self, direction):\n",
    "        \"\"\"Attempt to move in the specified direction.\"\"\"\n",
    "        if self.current_pos is None:\n",
    "            print(\"Initial position not set.\")\n",
    "            return 0\n",
    "\n",
    "        row, col = self.current_pos\n",
    "        if direction == \"up\":\n",
    "            new_pos = (row - 1, col)\n",
    "        elif direction == \"down\":\n",
    "            new_pos = (row + 1, col)\n",
    "        elif direction == \"left\":\n",
    "            new_pos = (row, col - 1)\n",
    "        elif direction == \"right\":\n",
    "            new_pos = (row, col + 1)\n",
    "        else:\n",
    "            print(\"Invalid direction.\")\n",
    "            return 0\n",
    "\n",
    "        if self.isValidMove(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "            self.set_point(*self.start_pos, [1 - self.layout[self.start_pos]])  # Reset start position color\n",
    "            self.start_pos = new_pos\n",
    "            self.set_point(*new_pos, [0, 0, 1])  # Mark new position with blue\n",
    "            if self.current_pos == self.exit_pos:\n",
    "                # print(\"Success! You've found the way out.\")\n",
    "                return 1\n",
    "        # else:\n",
    "        #    print(\"Invalid move: Can't move through walls or out of bounds.\")\n",
    "\n",
    "    def isValidMove(self, pos):\n",
    "        \"\"\"Check if a move is valid (within bounds and not through walls).\"\"\"\n",
    "        row, col = pos\n",
    "        if 0 <= row < self.layout.shape[0] and 0 <= col < self.layout.shape[1]:\n",
    "            return self.layout[row, col] == 0\n",
    "        return False\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the maze.\"\"\"\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(self.rgb_maze, interpolation='nearest')\n",
    "        plt.xticks([]), plt.yticks([])  # Hide axes ticks\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 700x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of MazeGame\n",
    "maze_game = MazeGame()\n",
    "\n",
    "# Set the initial and exit positions\n",
    "maze_game.initialPosition(3, 3)\n",
    "maze_game.setExit(5, 6)\n",
    "maze_game.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model-Free RL agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class QLearningAgent_with_noisy_punishment:\n",
    "    def __init__(self, game, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.game = game\n",
    "        self.q_table = np.zeros((game.layout.size, 4))  # 4 actions: up, down, left, right\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Converts a 2D state to a 1D index for the Q-table.\"\"\"\n",
    "        return state[0] * self.game.layout.shape[1] + state[1]\n",
    "\n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"Choose action based on ε-greedy policy.\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state_index])]  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value using the Q-learning formula.\"\"\"\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        action_index = self.actions.index(action)\n",
    "        next_max = np.max(self.q_table[next_state_index])\n",
    "        self.q_table[state_index, action_index] = self.q_table[state_index, action_index] + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - self.q_table[state_index, action_index])\n",
    "\n",
    "    def train(self, episodes=1000, print_out_internal_variable=False):\n",
    "        for episode in range(episodes):\n",
    "            self.game.initialPosition(3, 3)  # Reset to starting position\n",
    "            state = self.game.current_pos\n",
    "            steps = 0\n",
    "            success = 0\n",
    "            while not success and steps < 50:\n",
    "                action = self.choose_action(self.state_to_index(state))\n",
    "                old_state = state\n",
    "                success = self.game.makeMove(action)\n",
    "                state = self.game.current_pos\n",
    "\n",
    "                # Extremely important note here: There are generally two ways that we could design out Q learner here\n",
    "                # First, we could use memory reply and only update the Q table if the agent has reached the exit at\n",
    "                # the end. In this case, the actions of the agent is imagined as all the possible action trajectories\n",
    "                # of the agent. I personally believe that this supposedly is the correct way to build the agent.\n",
    "                # However, there is another way to build the agent. That we encode each pixel as a possible state.\n",
    "                # Then, each state has 4 possible actions. We build a 4 by 49 matrix. An action would lead to a\n",
    "                # punishment, if the agent does not move. That means, moving towards a wall would lead to negative\n",
    "                # rewards. In this way, we could deal away with the high-dimensionality of the action trajectory space.\n",
    "                # However, technically, we don't have a sparse reward signal for our Q learning agent. Instead,\n",
    "                # we have a very dense rewarded-punished Q learning agent.\n",
    "                #\n",
    "                # Nevertheless, given this really hacked trick, the Q learning agent still takes many episodes to\n",
    "                # finally actually learned how to navigate out of the maze.\n",
    "\n",
    "                # Specifically, we reward if the agent reaches the exit, and punishes if the agent does not move.\n",
    "                noise_level=0.05\n",
    "                noise = np.random.uniform(-noise_level, noise_level)\n",
    "                reward = 1 if success else -0.0012 + noise # Simple reward: -0.5 for each step, +1 for reaching the goal\n",
    "\n",
    "                # we implement with actual sparse reward\n",
    "                # reward = 1 if success else 0  # Simple reward: +1 for reaching the goal\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.learn(old_state, action, reward, state)\n",
    "                steps += 1\n",
    "            if print_out_internal_variable:\n",
    "                print(f\"Episode {episode}: the agent took {steps} steps.\")\n",
    "                print('Current Q table,', self.q_table)\n",
    "                print('The agent reaches the exit or not?', success)\n",
    "            # if episode % 100000 == 0:\n",
    "            #    print(f\"in Episode {episode}: the agent completed in {steps} steps.\")\n",
    "            # if success:\n",
    "                # print(f\"in Episode {episode}: the agent completed in {steps} steps.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LogScale Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test_agent_for_log_scale_experiment(agent_for_test, game, max_steps=50):\n",
    "    game.initialPosition(3, 3)  # Reset to starting position\n",
    "    state = game.current_pos\n",
    "    steps = 0\n",
    "    success = False\n",
    "    while not success and steps < max_steps:\n",
    "        state_index = agent_for_test.state_to_index(state)\n",
    "        action = agent_for_test.actions[np.argmax(agent_for_test.q_table[state_index])]  # Choose best action\n",
    "        success = game.makeMove(action)  # Perform the action\n",
    "        state = game.current_pos  # Update state\n",
    "        steps += 1\n",
    "    if success:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def experiment(maze_game, log_scale=1, num_agents=10):\n",
    "    train_episode = 10**log_scale\n",
    "    print('Training Trials for each agent:', train_episode)\n",
    "    success_agents = 0\n",
    "    for _ in range(num_agents):\n",
    "        agent_for_experiment = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "        agent_for_experiment.train(train_episode, False)\n",
    "        successes = test_agent_for_log_scale_experiment(agent_for_experiment, maze_game)\n",
    "        if successes:\n",
    "            success_agents = success_agents + 1\n",
    "    average_success_percentage = success_agents/num_agents\n",
    "    print('The successfully navigated agents:', success_agents)\n",
    "    print(f\"Average success percentage across {num_agents} agents: {average_success_percentage*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Do experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Trials for each agent: 10\n",
      "The successfully navigated agents: 0\n",
      "Average success percentage across 10 agents: 0.00%\n",
      "Training Trials for each agent: 100\n",
      "The successfully navigated agents: 0\n",
      "Average success percentage across 10 agents: 0.00%\n",
      "Training Trials for each agent: 1000\n",
      "The successfully navigated agents: 0\n",
      "Average success percentage across 10 agents: 0.00%\n",
      "Training Trials for each agent: 10000\n",
      "The successfully navigated agents: 3\n",
      "Average success percentage across 10 agents: 30.00%\n",
      "Training Trials for each agent: 100000\n",
      "The successfully navigated agents: 6\n",
      "Average success percentage across 10 agents: 60.00%\n",
      "Training Trials for each agent: 1000000\n",
      "The successfully navigated agents: 6\n",
      "Average success percentage across 10 agents: 60.00%\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 7):\n",
    "    # 10 to the i 10^i trials for training\n",
    "    experiment(maze_game, i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment for running 10^7 training trials"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_episode = 10**7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Training Trials for each agent:', train_episode)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Monitor the Agent Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent_with_noisy_punishment:\n",
    "    def __init__(self, game, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.game = game\n",
    "        self.q_table = np.zeros((game.layout.size, 4))  # 4 actions: up, down, left, right\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Converts a 2D state to a 1D index for the Q-table.\"\"\"\n",
    "        return state[0] * self.game.layout.shape[1] + state[1]\n",
    "\n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"Choose action based on ε-greedy policy.\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state_index])]  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value using the Q-learning formula.\"\"\"\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        action_index = self.actions.index(action)\n",
    "        next_max = np.max(self.q_table[next_state_index])\n",
    "        self.q_table[state_index, action_index] = self.q_table[state_index, action_index] + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - self.q_table[state_index, action_index])\n",
    "\n",
    "    def train(self, episodes=1000, print_out_internal_variable=False):\n",
    "        for episode in range(episodes):\n",
    "            self.game.initialPosition(3, 3)  # Reset to starting position\n",
    "            state = self.game.current_pos\n",
    "            steps = 0\n",
    "            success = 0\n",
    "            while not success and steps < 50:\n",
    "                action = self.choose_action(self.state_to_index(state))\n",
    "                old_state = state\n",
    "                success = self.game.makeMove(action)\n",
    "                state = self.game.current_pos\n",
    "\n",
    "                # Extremely important note here: There are generally two ways that we could design out Q learner here\n",
    "                # First, we could use memory reply and only update the Q table if the agent has reached the exit at\n",
    "                # the end. In this case, the actions of the agent is imagined as all the possible action trajectories\n",
    "                # of the agent. I personally believe that this supposedly is the correct way to build the agent.\n",
    "                # However, there is another way to build the agent. That we encode each pixel as a possible state.\n",
    "                # Then, each state has 4 possible actions. We build a 4 by 49 matrix. An action would lead to a\n",
    "                # punishment, if the agent does not move. That means, moving towards a wall would lead to negative\n",
    "                # rewards. In this way, we could deal away with the high-dimensionality of the action trajectory space.\n",
    "                # However, technically, we don't have a sparse reward signal for our Q learning agent. Instead,\n",
    "                # we have a very dense rewarded-punished Q learning agent.\n",
    "                #\n",
    "                # Nevertheless, given this really hacked trick, the Q learning agent still takes many episodes to\n",
    "                # finally actually learned how to navigate out of the maze.\n",
    "\n",
    "                # Specifically, we reward if the agent reaches the exit, and punishes if the agent does not move.\n",
    "                noise_level=0.05\n",
    "                noise = np.random.uniform(-noise_level, noise_level)\n",
    "                reward = 1 if success else -0.001 + noise # Simple reward: -0.5 for each step, +1 for reaching the goal\n",
    "\n",
    "                # we implement with actual sparse reward\n",
    "                # reward = 1 if success else 0  # Simple reward: +1 for reaching the goal\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.learn(old_state, action, reward, state)\n",
    "                steps += 1\n",
    "            if print_out_internal_variable:\n",
    "                print(f\"Episode {episode}: the agent took {steps} steps.\")\n",
    "                print('Current Q table,', self.q_table)\n",
    "                print('The agent reaches the exit or not?', success)\n",
    "            if episode % 100000 == 0:\n",
    "                print(f\"in Episode {episode}: the agent completed in {steps} steps.\")\n",
    "            # if success:\n",
    "                # print(f\"in Episode {episode}: the agent completed in {steps} steps.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_1 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_1.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_1, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_2 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_2.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_2, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_3 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_3.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_3, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_4 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_4.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_4, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_5 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_5.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_5, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_6 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_6.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_6, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_7 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_7.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_7, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_8 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_8.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_8, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_9 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_9.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_9, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent_10 = QLearningAgent_with_noisy_punishment(maze_game)\n",
    "agent_10.train(train_episode, False)\n",
    "successes = test_agent_for_log_scale_experiment(agent_10, maze_game)\n",
    "if successes:\n",
    "    print('This agent has successfully navigated out of the maze')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
