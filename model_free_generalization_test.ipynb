{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Game Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class MazeGame:\n",
    "    def __init__(self):\n",
    "        maze_layout = [[1, 1, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 0, 1],\n",
    "               [1, 0, 1, 0, 0, 0, 1],\n",
    "               [1, 0, 1, 1, 1, 1, 1],\n",
    "               [1, 0, 0, 0, 0, 0, 0],\n",
    "               [1, 1, 1, 1, 1, 1, 1]]\n",
    "        \"\"\"Initialize the maze game with a given layout.\"\"\"\n",
    "        self.layout = np.array(maze_layout)\n",
    "        self.rgb_maze = self._to_rgb()\n",
    "        self.start_pos = None\n",
    "        self.exit_pos = None\n",
    "        self.current_pos = None\n",
    "\n",
    "    def _to_rgb(self):\n",
    "        \"\"\"Convert the maze layout to an RGB image.\"\"\"\n",
    "        rgb = np.zeros((*self.layout.shape, 3))  # Create a new RGB array\n",
    "        for i in range(3):  # Copy the grayscale values into each RGB channel\n",
    "            rgb[:, :, i] = 1 - self.layout\n",
    "        return rgb\n",
    "\n",
    "    def initialPosition(self, row, col):\n",
    "        \"\"\"Set the initial position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.start_pos = (row, col)\n",
    "            self.current_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 0, 1])  # Mark initial position with blue\n",
    "        else:\n",
    "            print(\"Invalid initial position: It is on a wall.\")\n",
    "\n",
    "    def setExit(self, row, col):\n",
    "        \"\"\"Set the exit position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.exit_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 1, 0])  # Mark exit position with green\n",
    "        else:\n",
    "            print(\"Invalid exit position: It is on a wall.\")\n",
    "\n",
    "    def set_point(self, row, col, color):\n",
    "        \"\"\"Set a specific point in the maze to a given color.\"\"\"\n",
    "        self.rgb_maze[row, col] = color\n",
    "\n",
    "    def makeMove(self, direction):\n",
    "        \"\"\"Attempt to move in the specified direction.\"\"\"\n",
    "        if self.current_pos is None:\n",
    "            print(\"Initial position not set.\")\n",
    "            return 0\n",
    "\n",
    "        row, col = self.current_pos\n",
    "        if direction == \"up\":\n",
    "            new_pos = (row - 1, col)\n",
    "        elif direction == \"down\":\n",
    "            new_pos = (row + 1, col)\n",
    "        elif direction == \"left\":\n",
    "            new_pos = (row, col - 1)\n",
    "        elif direction == \"right\":\n",
    "            new_pos = (row, col + 1)\n",
    "        else:\n",
    "            print(\"Invalid direction.\")\n",
    "            return 0\n",
    "\n",
    "        if self.isValidMove(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "            self.set_point(*self.start_pos, [1 - self.layout[self.start_pos]])  # Reset start position color\n",
    "            self.start_pos = new_pos\n",
    "            self.set_point(*new_pos, [0, 0, 1])  # Mark new position with blue\n",
    "            if self.current_pos == self.exit_pos:\n",
    "                # print(\"Success! You've found the way out.\")\n",
    "                return 1\n",
    "        # else:\n",
    "        #    print(\"Invalid move: Can't move through walls or out of bounds.\")\n",
    "\n",
    "    def isValidMove(self, pos):\n",
    "        \"\"\"Check if a move is valid (within bounds and not through walls).\"\"\"\n",
    "        row, col = pos\n",
    "        if 0 <= row < self.layout.shape[0] and 0 <= col < self.layout.shape[1]:\n",
    "            return self.layout[row, col] == 0\n",
    "        return False\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the maze.\"\"\"\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(self.rgb_maze, interpolation='nearest')\n",
    "        plt.xticks([]), plt.yticks([])  # Hide axes ticks\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 700x700 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of MazeGame\n",
    "maze_game = MazeGame()\n",
    "\n",
    "# Set the initial and exit positions\n",
    "maze_game.initialPosition(3, 3)\n",
    "maze_game.setExit(5, 6)\n",
    "maze_game.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reversed Maze"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class Reversed_MazeGame:\n",
    "    def __init__(self):\n",
    "        maze_layout = [ [1, 1, 1, 1, 1, 1, 1],\n",
    "                        [0, 0, 0, 0, 0, 0, 1],\n",
    "                        [1, 1, 1, 1, 1, 0, 1],\n",
    "                        [1, 0, 0, 0, 1, 0, 1],\n",
    "                        [1, 0, 1, 1, 1, 0, 1],\n",
    "                        [1, 0, 0, 0, 0, 0, 1],\n",
    "                        [1, 1, 1, 1, 1, 1, 1]]\n",
    "        \"\"\"Initialize the maze game with a given layout.\"\"\"\n",
    "        self.layout = np.array(maze_layout)\n",
    "        self.rgb_maze = self._to_rgb()\n",
    "        self.start_pos = None\n",
    "        self.exit_pos = None\n",
    "        self.current_pos = None\n",
    "\n",
    "    def _to_rgb(self):\n",
    "        \"\"\"Convert the maze layout to an RGB image.\"\"\"\n",
    "        rgb = np.zeros((*self.layout.shape, 3))  # Create a new RGB array\n",
    "        for i in range(3):  # Copy the grayscale values into each RGB channel\n",
    "            rgb[:, :, i] = 1 - self.layout\n",
    "        return rgb\n",
    "\n",
    "    def initialPosition(self, row, col):\n",
    "        \"\"\"Set the initial position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.start_pos = (row, col)\n",
    "            self.current_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 0, 1])  # Mark initial position with blue\n",
    "        else:\n",
    "            print(\"Invalid initial position: It is on a wall.\")\n",
    "\n",
    "    def setExit(self, row, col):\n",
    "        \"\"\"Set the exit position in the maze.\"\"\"\n",
    "        if self.layout[row, col] == 0:\n",
    "            self.exit_pos = (row, col)\n",
    "            self.set_point(row, col, [0, 1, 0])  # Mark exit position with green\n",
    "        else:\n",
    "            print(\"Invalid exit position: It is on a wall.\")\n",
    "\n",
    "    def set_point(self, row, col, color):\n",
    "        \"\"\"Set a specific point in the maze to a given color.\"\"\"\n",
    "        self.rgb_maze[row, col] = color\n",
    "\n",
    "    def makeMove(self, direction):\n",
    "        \"\"\"Attempt to move in the specified direction.\"\"\"\n",
    "        if self.current_pos is None:\n",
    "            print(\"Initial position not set.\")\n",
    "            return 0\n",
    "\n",
    "        row, col = self.current_pos\n",
    "        if direction == \"up\":\n",
    "            new_pos = (row - 1, col)\n",
    "        elif direction == \"down\":\n",
    "            new_pos = (row + 1, col)\n",
    "        elif direction == \"left\":\n",
    "            new_pos = (row, col - 1)\n",
    "        elif direction == \"right\":\n",
    "            new_pos = (row, col + 1)\n",
    "        else:\n",
    "            print(\"Invalid direction.\")\n",
    "            return 0\n",
    "\n",
    "        if self.isValidMove(new_pos):\n",
    "            self.current_pos = new_pos\n",
    "            self.set_point(*self.start_pos, [1 - self.layout[self.start_pos]])  # Reset start position color\n",
    "            self.start_pos = new_pos\n",
    "            self.set_point(*new_pos, [0, 0, 1])  # Mark new position with blue\n",
    "            if self.current_pos == self.exit_pos:\n",
    "                # print(\"Success! You've found the way out.\")\n",
    "                return 1\n",
    "        # else:\n",
    "        #    print(\"Invalid move: Can't move through walls or out of bounds.\")\n",
    "\n",
    "    def isValidMove(self, pos):\n",
    "        \"\"\"Check if a move is valid (within bounds and not through walls).\"\"\"\n",
    "        row, col = pos\n",
    "        if 0 <= row < self.layout.shape[0] and 0 <= col < self.layout.shape[1]:\n",
    "            return self.layout[row, col] == 0\n",
    "        return False\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the maze.\"\"\"\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(self.rgb_maze, interpolation='nearest')\n",
    "        plt.xticks([]), plt.yticks([])  # Hide axes ticks\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 700x700 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAIvCAYAAABTFlB6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKiUlEQVR4nO3asW7bShRF0RlBLZNesP//wwywD9RrXvGaFHGsAHLorazVisWBqo1LzrXWGgAAAaejBwAA3Eu4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGSc73nodruNfd/Htm1jzvnZmwCAf8xaa1yv13G5XMbp9P5d5a5w2fd9vL6+PmwcAMCvvL29jZeXl3d/v+tV0bZtDxsEAPCej5rjrnDxeggA+Bs+ag4f5wIAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMg4/9HTP8YY3z5nyDNaYx09AYCDzDmPnvCUXFwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZ5z96+vsnrXhSc8yjJwA8xFrr6AkwxnBxAQBChAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZJyPHvDM1lpHT+AfMOfRC4r8aVDl4gIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMg4Hz0AfjbnPHoCAF+YiwsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACDjfPQA+Nla6+gJAHxhLi4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAICM89EDntmc8+gJAPBUXFwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIEO4AAAZwgUAyBAuAECGcAEAMoQLAJAhXACADOECAGQIFwAgQ7gAABnCBQDIEC4AQIZwAQAyhAsAkCFcAIAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADKECwCQIVwAgAzhAgBkCBcAIOOucFlrffYOAIAPm+OucLlerw8ZAwDwOx81x1x3nFNut9vY931s2zbmnA8bBwAwxv+Xluv1Oi6Xyzid3r+r3BUuAABfgY9zAYAM4QIAZAgXACBDuAAAGcIFAMgQLgBAhnABADL+A8jmRnfKeeWxAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of MazeGame\n",
    "reversed_maze_game = Reversed_MazeGame()\n",
    "\n",
    "# Set the initial and exit positions\n",
    "reversed_maze_game.initialPosition(3, 3)\n",
    "reversed_maze_game.setExit(1, 0)\n",
    "reversed_maze_game.plot()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model-Free RL agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class QLearningAgent_with_noisy_punishment:\n",
    "    def __init__(self, game, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.game = game\n",
    "        self.q_table = np.zeros((game.layout.size, 4))  # 4 actions: up, down, left, right\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Converts a 2D state to a 1D index for the Q-table.\"\"\"\n",
    "        return state[0] * self.game.layout.shape[1] + state[1]\n",
    "\n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"Choose action based on Îµ-greedy policy.\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state_index])]  # Exploit\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-value using the Q-learning formula.\"\"\"\n",
    "        state_index = self.state_to_index(state)\n",
    "        next_state_index = self.state_to_index(next_state)\n",
    "        action_index = self.actions.index(action)\n",
    "        next_max = np.max(self.q_table[next_state_index])\n",
    "        self.q_table[state_index, action_index] = self.q_table[state_index, action_index] + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - self.q_table[state_index, action_index])\n",
    "\n",
    "    def train(self, episodes=1000, print_out_internal_variable=False):\n",
    "        for episode in range(episodes):\n",
    "            self.game.initialPosition(3, 3)  # Reset to starting position\n",
    "            state = self.game.current_pos\n",
    "            steps = 0\n",
    "            success = 0\n",
    "            while not success and steps < 50:\n",
    "                action = self.choose_action(self.state_to_index(state))\n",
    "                old_state = state\n",
    "                success = self.game.makeMove(action)\n",
    "                state = self.game.current_pos\n",
    "\n",
    "                # Extremely important note here: There are generally two ways that we could design out Q learner here\n",
    "                # First, we could use memory reply and only update the Q table if the agent has reached the exit at\n",
    "                # the end. In this case, the actions of the agent is imagined as all the possible action trajectories\n",
    "                # of the agent. I personally believe that this supposedly is the correct way to build the agent.\n",
    "                # However, there is another way to build the agent. That we encode each pixel as a possible state.\n",
    "                # Then, each state has 4 possible actions. We build a 4 by 49 matrix. An action would lead to a\n",
    "                # punishment, if the agent does not move. That means, moving towards a wall would lead to negative\n",
    "                # rewards. In this way, we could deal away with the high-dimensionality of the action trajectory space.\n",
    "                # However, technically, we don't have a sparse reward signal for our Q learning agent. Instead,\n",
    "                # we have a very dense rewarded-punished Q learning agent.\n",
    "                #\n",
    "                # Nevertheless, given this really hacked trick, the Q learning agent still takes many episodes to\n",
    "                # finally actually learned how to navigate out of the maze.\n",
    "\n",
    "                # Specifically, we reward if the agent reaches the exit, and punishes if the agent does not move.\n",
    "                noise_level=0.05\n",
    "                noise = np.random.uniform(-noise_level, noise_level)\n",
    "                reward = 1 if success else -0.0014 + noise # Simple reward: -0.5 for each step, +1 for reaching the goal\n",
    "\n",
    "                # we implement with actual sparse reward\n",
    "                # reward = 1 if success else 0  # Simple reward: +1 for reaching the goal\n",
    "\n",
    "                # Learn from the transition\n",
    "                self.learn(old_state, action, reward, state)\n",
    "                steps += 1\n",
    "            if print_out_internal_variable:\n",
    "                print(f\"Episode {episode}: the agent took {steps} steps.\")\n",
    "                print('Current Q table,', self.q_table)\n",
    "                print('The agent reaches the exit or not?', success)\n",
    "            # if episode % 100000 == 0:\n",
    "            #    print(f\"in Episode {episode}: the agent completed in {steps} steps.\")\n",
    "            # if success:\n",
    "                # print(f\"in Episode {episode}: the agent completed in {steps} steps.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reverse Maze Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def test_agent_for_original_maze(agent_for_test, game, max_steps=50):\n",
    "    game.initialPosition(3, 3)  # Reset to starting position\n",
    "    state = game.current_pos\n",
    "    steps = 0\n",
    "    success = False\n",
    "    while not success and steps < max_steps:\n",
    "        state_index = agent_for_test.state_to_index(state)\n",
    "        action = agent_for_test.actions[np.argmax(agent_for_test.q_table[state_index])]  # Choose best action\n",
    "        success = game.makeMove(action)  # Perform the action\n",
    "        state = game.current_pos  # Update state\n",
    "        steps += 1\n",
    "    if success:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def maze_experiment(maze, reversed_maze,log_scale=1, num_agents=10):\n",
    "    train_episode = 10**log_scale\n",
    "    print('Training Trials for each agent:', train_episode)\n",
    "    success_agents = 0\n",
    "    for _ in range(num_agents):\n",
    "        agent_for_experiment = QLearningAgent_with_noisy_punishment(maze)\n",
    "        agent_for_experiment.train(train_episode, False)\n",
    "        successes = test_agent_for_original_maze(agent_for_experiment, maze)\n",
    "        if successes:\n",
    "            success_agents = success_agents + 1\n",
    "    average_success_percentage = success_agents/num_agents\n",
    "    print('The successfully navigated agents:', success_agents)\n",
    "    print(f\"Average success percentage across {num_agents} agents: {average_success_percentage*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Original and Reversed Maze experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Trials for each agent: 1000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m original_maze_experiment(maze_game,\u001B[38;5;241m6\u001B[39m)\n",
      "Cell \u001B[0;32mIn[30], line 7\u001B[0m, in \u001B[0;36moriginal_maze_experiment\u001B[0;34m(maze_game, log_scale, num_agents)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_agents):\n\u001B[1;32m      6\u001B[0m     agent_for_experiment \u001B[38;5;241m=\u001B[39m QLearningAgent_with_noisy_punishment(maze_game)\n\u001B[0;32m----> 7\u001B[0m     agent_for_experiment\u001B[38;5;241m.\u001B[39mtrain(train_episode, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      8\u001B[0m     successes \u001B[38;5;241m=\u001B[39m test_agent_for_original_maze(agent_for_experiment, maze_game)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m successes:\n",
      "Cell \u001B[0;32mIn[28], line 65\u001B[0m, in \u001B[0;36mQLearningAgent_with_noisy_punishment.train\u001B[0;34m(self, episodes, print_out_internal_variable)\u001B[0m\n\u001B[1;32m     59\u001B[0m     reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m success \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.0014\u001B[39m \u001B[38;5;241m+\u001B[39m noise \u001B[38;5;66;03m# Simple reward: -0.5 for each step, +1 for reaching the goal\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# we implement with actual sparse reward\u001B[39;00m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# reward = 1 if success else 0  # Simple reward: +1 for reaching the goal\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Learn from the transition\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearn(old_state, action, reward, state)\n\u001B[1;32m     66\u001B[0m     steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m print_out_internal_variable:\n",
      "Cell \u001B[0;32mIn[28], line 26\u001B[0m, in \u001B[0;36mQLearningAgent_with_noisy_punishment.learn\u001B[0;34m(self, state, action, reward, next_state)\u001B[0m\n\u001B[1;32m     24\u001B[0m next_state_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate_to_index(next_state)\n\u001B[1;32m     25\u001B[0m action_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactions\u001B[38;5;241m.\u001B[39mindex(action)\n\u001B[0;32m---> 26\u001B[0m next_max \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_table[next_state_index])\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_table[state_index, action_index] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_table[state_index, action_index] \u001B[38;5;241m+\u001B[39m \\\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m (reward \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m next_max \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_table[state_index, action_index])\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mamax\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2820\u001B[0m, in \u001B[0;36mamax\u001B[0;34m(a, axis, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2703\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_amax_dispatcher)\n\u001B[1;32m   2704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mamax\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2705\u001B[0m          where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n\u001B[1;32m   2706\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2707\u001B[0m \u001B[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001B[39;00m\n\u001B[1;32m   2708\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2818\u001B[0m \u001B[38;5;124;03m    5\u001B[39;00m\n\u001B[1;32m   2819\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2820\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapreduction(a, np\u001B[38;5;241m.\u001B[39mmaximum, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m'\u001B[39m, axis, \u001B[38;5;28;01mNone\u001B[39;00m, out,\n\u001B[1;32m   2821\u001B[0m                           keepdims\u001B[38;5;241m=\u001B[39mkeepdims, initial\u001B[38;5;241m=\u001B[39minitial, where\u001B[38;5;241m=\u001B[39mwhere)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ufunc\u001B[38;5;241m.\u001B[39mreduce(obj, axis, dtype, out, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "maze_experiment(maze_game,6)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
